---
title: "Run_forest_non-forest_model"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Run_forest_non-forest_model}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, eval = FALSE}
library(PEMr)

library(PEMprepr)
library(PEMsamplr)
library(PEMmodelr)

```


# Generate Forest/Non-Forest model

This workflow produces a model for broad groupings (forest/non-forest/non-vegetated). It provides the basis on which the forested model (See vignette: Run_forest_model_05) and non-forest models will be used. In effect it provides a mask to clip other models. These steps are explained in more details in the vignettes: Run_forest_model_05. 

The workflow for the forest/non-forest split will step through the same process as the forest model but with a different attribute and more simple process. 



## 1.1 prep all training points

Prepare training points for using the attribute "mapunit_fnf" in the mapkey.csv. This differs to the forest only mapping. 

As previously we will generate a full list of covariates. 

```{r, eval = FALSE}

out_dir = fs::path(PEMprepr::read_fid()$dir_3020_draft$path_rel, "10_fnf")

covarkey = read.csv(fs::path(PEMprepr::read_fid()$dir_30_model$path_rel,"covar_key.csv"))

tpts <- prep_tps(
  allpts = sf::st_read(fs::path(PEMprepr::read_fid()$dir_20105030_attributed_field_data$path_rel, "allpoints_att.gpkg")),
  mapkey = read.csv(fs::path(PEMprepr::read_fid()$dir_30_model$path_abs,"mapunitkey_final.csv")),
  covarkey = read.csv(fs::path(PEMprepr::read_fid()$dir_30_model$path_rel,"covar_key.csv")),
  attribute = "mapunit_fnf",
  bec = sf::st_read(fs::path(PEMprepr::read_fid()$dir_1010_vector$path_rel, "bec.gpkg")),
  min_no = 20)

core_names  <- covarkey |>
  dplyr::filter(type == "core") |>
  dplyr::select(value) |> dplyr::pull()


# get full list of covariates
mcols <- names(tpts)[!names(tpts) %in% c(core_names)]
saveRDS(mcols, fs::path(out_dir, "full_covariate_list.rds"))


# convert to csv
tpts <- cbind(tpts, as.data.frame(sf::st_coordinates(tpts))) |>
  sf::st_drop_geometry()

write.csv(tpts, fs::path(out_dir, "training_pts.csv"), row.names = FALSE)

```


## 1.2) Generate a fuzzy matrix. 

To assist in accuracy assessment we need to generate a fuzzy metrics. This provides a list of all mapunits  we need to create a table which assigns partial correct values for mapunit calls that are similar (on the edatopic position) to the correct calls. In this case, scores could be awarded for on a sliding scale from 1 (Correct) to 0 (nowhere close) with partial credit assigned to closely related mapunits. Note this requires a matrix which specifies the similarity between all combinations of possible calls. 

The final output is a list of all mapunits within the tpts$mapunit1 column and cross reference with all other units with an equivalent value between 0 and 1. The dataframe consists of three columns: target, Pred, f. 

Note for this model the types include forest/non-forest and non-vegetated so we have a simple fuzzy matrix. Might not need this as "nonforest and forest cross over = 0"

```{r}
#| eval: false
#| 
# this is currently a placeholder that Claire is working on.
# it is possible to use the existing fuzzy_matrix.csv files until this is completed

#fuzzy_matrix <- read.csv(fs::path("fuzzy_matrix.csv" ))
#utils::write.csv(fuzzy_matrix, fs::path(out_dir, "fuzzy_matrix.csv"), row.names = FALSE)

```



## 1.3)  Recursive Feature selection / Correlated variable reduction

To improve the interpretability of the models, we can firstly remove the correlated covariates. While there are several methods available to complete this task we chose a simple correlation matrix with a cutoff values of 0.9.  This is based on the entire raster surface (not just the points). The cutoff parameters can be adjusted. 

In the forest/non-forest models we also use the covariates grouped as "dem" and "structure".

```{r}
#| eval: false

mcols <- readRDS(fs::path(out_dir, "full_covariate_list.rds"))

reduced_vars <- reduce_features(mcols,
                                covar_dir = fs::path(PEMprepr::read_fid()$dir_1020_covariates$path_rel, "5m"),
                                covarkey = covarkey,
                                covtype = c("dem", "structure"),
                                cutoff = 0.90)

write.csv(reduced_vars, fs::path(out_dir, "reduced_covariate_list.csv"), row.names = FALSE)

```

## 1.4) prepare the training point models (fnf)

```{r}
#| eval: false

tpts <- read.csv(fs::path(out_dir, "training_pts.csv"))

model_bgc <- prep_model_tps(
  prepped_points = tpts,
  covars = reduced_vars,
  model_type = "fnf",
  out_dir = out_dir
)


```


# 1.5) determine optimum parameters for the model

We use only the original data points (not the neighbours) and pure calls to tune the model hyperparameters. This includes estimating mtry and min_n to use for the models. Best accuracy is determined based on the roc_auc and accuracy metrics. We can also plot the each is exported to enable more assessment of best options. 
```{r}
#| eval: false

tune_bgc <- tune_model_params(
  prepped_points = model_bgc,
  covars = reduced_vars,
  min_no = 20,
  out_dir = out_dir,
  output_type = "best",
  accuracy_type = "roc_auc"
)

```

## 2) Run the base model

```{r}
#| eval: false

#This runs the model with no balancing using the parameters generated above
out_dir = fs::path(PEMprepr::read_fid()$dir_3020_draft$path_rel, "10_fnf")

# read in data
bgc_pts_subzone <- readRDS(fs::path(out_dir, "model_input_pts.rds"))

# read in fuzzy matrix
fuzz_matrix <- read.csv(file.path(out_dir, "fuzzy_matrix.csv" )) |>
  dplyr::select(target, Pred, fVal)

covars = read.csv(fs::path(out_dir, "reduced_covariate_list.csv")) |> dplyr::pull()

run_base_model(
  bgc_pts_subzone = bgc_pts_subzone,
  fuzz_matrix = fuzz_matrix,
  covars = covars,
  use_neighbours = FALSE,
  detailed_output = TRUE,
  out_dir = out_dir)

```


######################
# 3) Balance?
######################



# 4) Run final model

In this example we use the base model as the final model. This could also be a balanced option. 

```{r}
#| eval: false

out_dir = fs::path(PEMprepr::read_fid()$dir_3020_draft$path_rel, "10_fnf")
train_data <- readRDS(fs::path(out_dir, "model_input_pts.rds"))
covars <- read.csv(fs::path(out_dir, "reduced_covariate_list.csv")) |> dplyr::pull()


final_mod <- run_final_model(train_data,
                             covars,
                             model_bal = "base",
                             report = FALSE,
                             out_dir = out_dir)

```


# 5) Predict maps

```{r}
#| eval: false

model_dir = fs::path(PEMprepr::read_fid()$dir_3020_draft$path_rel, "10_fnf")
covars = read.csv(fs::path(model_dir, "reduced_covariate_list.csv")) |>  dplyr::pull()
cov_dir = fs::path(PEMprepr::read_fid()$dir_1020_covariates$path_rel, "5m")
tile_dir = fs::path(PEMprepr::read_fid()$dir_30_model$path_rel,"tiles")
bec_shp <- sf::st_read(fs::path(PEMprepr::read_fid()$dir_1010_vector$path_rel,"bec.gpkg"), quiet = TRUE)

run_predict_map(
  model_type = "fnf",
  model_dir = model_dir,
  model_name = "final_model_base.rds",
  covars = covars,
  cov_dir = cov_dir,
  tile_dir = tile_dir,
  bec_shp = bec_shp
)

```

